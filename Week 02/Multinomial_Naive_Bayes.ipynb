{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd7ec4a",
   "metadata": {},
   "source": [
    "For a document with word counts x = (x₁, x₂, ..., xₙ) (where xᵢ is how many times word i appears), the probability of seeing this exact word count pattern if the document belongs to class Cₖ is:\n",
    "\n",
    "​$$\\log P(d|c) = \\underbrace{\\log \\left( \\frac{(\\sum x_i)!}{\\prod x_i!} \\right)}_{\\text{Constant } K} + \\sum x_i \\log P(x_i|c)$$\n",
    " \n",
    "​The Goal\n",
    "Prove that for classification, the Multinomial Naive Bayes posterior probability $P(C_k \\mid \\mathbf{x})$ is proportional to a linear combination of features:\n",
    "$$\\log P(C_k \\mid \\mathbf{x}) \\propto b_k + \\sum_{i=1}^{n} x_i w_{ki}$$\n",
    "Step 1: The Multinomial Likelihood\n",
    "We start with the actual probability of seeing a document (or count vector) $\\mathbf{x}$ given a class $C_k$:\n",
    "$$P(\\mathbf{x} \\mid C_k) = \\frac{(\\sum_{i=1}^{n} x_i)!}{\\prod_{i=1}^{n} x_i!} \\prod_{i=1}^{n} p_{ki}^{x_i}$$\n",
    "$\\sum x_i$Length of the document.\n",
    "$x_i$How many times word $i$ appears.\n",
    "$p_{ki}$How \"popular\" word $i$ is in Class $k$.\n",
    "$n!$The number of ways to arrange the counts.\n",
    "Where:$x_i$ is the count of feature $i$ in the sample.$p_{ki}$ is the probability of feature $i$ appearing in class $k$ (e.g., $P(word_i \\mid Class_k)$).\n",
    "Step 2: Bayes' RuleTo find the probability of the class given the data, we use Bayes' Theorem:\n",
    "$$P(C_k \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid C_k) P(C_k)}{P(\\mathbf{x})}$$\n",
    "In classification, we compare classes for the same input $\\mathbf{x}$. Therefore, the denominator $P(\\mathbf{x})$ is a constant for all $C_k$. We can ignore it:$$P(C_k \\mid \\mathbf{x}) \\propto P(\\mathbf{x} \\mid C_k) P(C_k)$$\n",
    "Step 3: Substitute the LikelihoodNow, substitute the full Multinomial formula into the proportion:\n",
    "$$P(C_k \\mid \\mathbf{x}) \\propto \\underbrace{\\left[ \\frac{(\\sum x_i)!}{\\prod x_i!} \\right]}_{\\text{Part A}} \\times \\underbrace{\\left[ \\prod_{i=1}^{n} p_{ki}^{x_i} \\right]}_{\\text{Part B}} \\times \\underbrace{P(C_k)}_{\\text{Part C}}$$\n",
    "Crucial Logic Step:\n",
    "Look at Part A. It consists entirely of $x_i$ (the counts in the input data). It does not contain $k$ (the class).Since Part A is the same value regardless of which class we are testing, it is a constant $K$ relative to the class comparison. We drop it.\n",
    "$$P(C_k \\mid \\mathbf{x}) \\propto P(C_k) \\prod_{i=1}^{n} p_{ki}^{x_i}$$\n",
    "Step 4: The Log TransformationTo turn this product into a linear sum (and to stay safe in PyTorch with small numbers), we take the natural logarithm of both sides:\n",
    "$$\\log P(C_k \\mid \\mathbf{x}) \\propto \\log \\left( P(C_k) \\prod_{i=1}^{n} p_{ki}^{x_i} \\right)$$\n",
    "Using the log identity $\\log(ab) = \\log a + \\log b$:$$\\log P(C_k \\mid \\mathbf{x}) \\propto \\log P(C_k) + \\log \\left( \\prod_{i=1}^{n} p_{ki}^{x_i} \\right)$$\n",
    "Using the log identity $\\log(a^b) = b \\log a$:$$\\log P(C_k \\mid \\mathbf{x}) \\propto \\log P(C_k) + \\sum_{i=1}^{n} x_i \\log p_{ki}$$\n",
    "Step 5: Defining the Linear WeightsNow, we map the terms to the standard linear form $y = b + wx$:\n",
    "1.Let the Bias be: $b_k = \\log P(C_k)$ (The Class Log Prior).\n",
    "2.Let the Weights be: $w_{ki} = \\log p_{ki}$ (The Feature Log Prob).Substituting these back in gives the final linear decision function:\n",
    "$$\\boxed{\\log P(C_k \\mid \\mathbf{x}) \\propto b_k + \\sum_{i=1}^{n} x_i w_{ki}}$$\n",
    "Final Interpretation for PyTorchWhen you write torch.matmul(X, W.t()) + b, you are performing exactly this:\n",
    "The sum $\\sum x_i w_{ki}$ is the dot product of your input vector and the class weights.\n",
    "The $b_k$ is the offset added at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Step 1: PyTorch MultinomialNB using label encoding\n",
    "\n",
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        y = torch.as_tensor(y)  # label-encoded 1D tensor\n",
    "\n",
    "        self.classes = torch.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Class priors\n",
    "        class_count = torch.tensor([(y == c).sum() for c in self.classes])\n",
    "        self.class_log_prior = torch.log(class_count / class_count.sum())\n",
    "\n",
    "        # Feature counts per class\n",
    "        feature_count = torch.zeros((n_classes, n_features), dtype=torch.float32)\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            feature_count[i] = X_c.sum(dim=0)\n",
    "\n",
    "        # Laplace smoothing\n",
    "        feature_count += self.alpha\n",
    "        self.feature_log_prob = torch.log(\n",
    "            feature_count / feature_count.sum(dim=1, keepdim=True)\n",
    "        )\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        return X @ self.feature_log_prob.T + self.class_log_prior\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        return torch.argmax(log_probs, dim=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        return torch.softmax(log_probs, dim=1)\n",
    "\n",
    "\n",
    "# Step 2: Raw text dataset\n",
    "\n",
    "messages = [\n",
    "    \"Win money now\",\n",
    "    \"Hello, how are you\",\n",
    "    \"Free offer just today\",\n",
    "    \"Let's meet tomorrow\",\n",
    "    \"Congratulations, you won\",\n",
    "    \"Are we still on for lunch\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=spam, 0=ham\n",
    "\n",
    "\n",
    "# Step 3: Scikit-learn preprocessing\n",
    "\n",
    "# Convert text to count vectors (Bag-of-Words)\n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(messages)  # sparse matrix\n",
    "X_dense = X_counts.toarray()  # convert to dense numpy array\n",
    "\n",
    "# Label encode labels (integers)\n",
    "encoder = LabelEncoder()\n",
    "y_labels = encoder.fit_transform(labels)  # [1,0,1,...] as integers\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_dense, y_labels, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Step 4: Train PyTorch MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Step 5: Predictions\n",
    "\n",
    "y_pred_classes = model.predict(X_test)\n",
    "y_pred_probs = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Predicted Classes:\", y_pred_classes.tolist())\n",
    "print(\"Predicted Probabilities:\\n\", y_pred_probs)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia 1.11",
   "language": "julia",
   "name": "julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
