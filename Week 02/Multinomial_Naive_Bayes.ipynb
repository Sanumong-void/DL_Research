{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302c07ab",
   "metadata": {},
   "source": [
    "For a document with word counts x = (x₁, x₂, ..., xₙ) (where xᵢ is how many times word i appears), the probability of seeing this exact word count pattern if the document belongs to class Cₖ is:\n",
    "\n",
    "​$$\\log P(d|c) = \\underbrace{\\log \\left( \\frac{(\\sum x_i)!}{\\prod x_i!} \\right)}_{\\text{Constant } K} + \\sum x_i \\log P(x_i|c)$$\n",
    " \n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7ec4a",
   "metadata": {},
   "source": [
    "For a document with word counts x = (x₁, x₂, ..., xₙ) (where xᵢ is how many times word i appears), the probability of seeing this exact word count pattern if the document belongs to class Cₖ is:\n",
    "\n",
    "​$$\\log P(d|c) = \\underbrace{\\log \\left( \\frac{(\\sum x_i)!}{\\prod x_i!} \\right)}_{\\text{Constant } K} + \\sum x_i \\log P(x_i|c)$$\n",
    " \n",
    "​The Goal\n",
    "Prove that for classification, the Multinomial Naive Bayes posterior probability $P(C_k \\mid \\mathbf{x})$ is proportional to a linear combination of features:\n",
    "$$\\log P(C_k \\mid \\mathbf{x}) \\propto b_k + \\sum_{i=1}^{n} x_i w_{ki}$$\n",
    "Step 1: The Multinomial Likelihood\n",
    "We start with the actual probability of seeing a document (or count vector) $\\mathbf{x}$ given a class $C_k$:\n",
    "$$P(\\mathbf{x} \\mid C_k) = \\frac{(\\sum_{i=1}^{n} x_i)!}{\\prod_{i=1}^{n} x_i!} \\prod_{i=1}^{n} p_{ki}^{x_i}$$\n",
    "Where:$x_i$ is the count of feature $i$ in the sample.$p_{ki}$ is the probability of feature $i$ appearing in class $k$ (e.g., $P(word_i \\mid Class_k)$).\n",
    "Step 2: Bayes' RuleTo find the probability of the class given the data, we use Bayes' Theorem:\n",
    "$$P(C_k \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid C_k) P(C_k)}{P(\\mathbf{x})}$$\n",
    "In classification, we compare classes for the same input $\\mathbf{x}$. Therefore, the denominator $P(\\mathbf{x})$ is a constant for all $C_k$. We can ignore it:$$P(C_k \\mid \\mathbf{x}) \\propto P(\\mathbf{x} \\mid C_k) P(C_k)$$\n",
    "Step 3: Substitute the LikelihoodNow, substitute the full Multinomial formula into the proportion:\n",
    "$$P(C_k \\mid \\mathbf{x}) \\propto \\underbrace{\\left[ \\frac{(\\sum x_i)!}{\\prod x_i!} \\right]}_{\\text{Part A}} \\times \\underbrace{\\left[ \\prod_{i=1}^{n} p_{ki}^{x_i} \\right]}_{\\text{Part B}} \\times \\underbrace{P(C_k)}_{\\text{Part C}}$$\n",
    "Crucial Logic Step:\n",
    "Look at Part A. It consists entirely of $x_i$ (the counts in the input data). It does not contain $k$ (the class).Since Part A is the same value regardless of which class we are testing, it is a constant $K$ relative to the class comparison. We drop it.\n",
    "$$P(C_k \\mid \\mathbf{x}) \\propto P(C_k) \\prod_{i=1}^{n} p_{ki}^{x_i}$$\n",
    "Step 4: The Log TransformationTo turn this product into a linear sum (and to stay safe in PyTorch with small numbers), we take the natural logarithm of both sides:\n",
    "$$\\log P(C_k \\mid \\mathbf{x}) \\propto \\log \\left( P(C_k) \\prod_{i=1}^{n} p_{ki}^{x_i} \\right)$$\n",
    "Using the log identity $\\log(ab) = \\log a + \\log b$:$$\\log P(C_k \\mid \\mathbf{x}) \\propto \\log P(C_k) + \\log \\left( \\prod_{i=1}^{n} p_{ki}^{x_i} \\right)$$\n",
    "Using the log identity $\\log(a^b) = b \\log a$:$$\\log P(C_k \\mid \\mathbf{x}) \\propto \\log P(C_k) + \\sum_{i=1}^{n} x_i \\log p_{ki}$$\n",
    "Step 5: Defining the Linear WeightsNow, we map the terms to the standard linear form $y = b + wx$:\n",
    "1.Let the Bias be: $b_k = \\log P(C_k)$ (The Class Log Prior).\n",
    "2.Let the Weights be: $w_{ki} = \\log p_{ki}$ (The Feature Log Prob).Substituting these back in gives the final linear decision function:\n",
    "$$\\boxed{\\log P(C_k \\mid \\mathbf{x}) \\propto b_k + \\sum_{i=1}^{n} x_i w_{ki}}$$\n",
    "Final Interpretation for PyTorchWhen you write torch.matmul(X, W.t()) + b, you are performing exactly this:\n",
    "The sum $\\sum x_i w_{ki}$ is the dot product of your input vector and the class weights.\n",
    "The $b_k$ is the offset added at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e4b51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
