# Perceptron - Neural Network Fundamentals
## Overview
A Perceptron is the fundamental building block of artificial neural networks. Introduced by Frank Rosenblatt in 1957, it represents the simplest form of a neural unit that makes binary decisions by combining inputs with learned weights and applying an activation function. Perceptrons are primarily used for binary classification tasks and form the foundational architecture for more complex deep learning models.

## Key Characteristics
- Binary Decision Making: Produces outputs of either 0 or 1
- Linear Classifier: Creates linear decision boundaries
- Supervised Learning: Learns from labeled training data
- Foundation Architecture: Basis for multi-layer neural networks

## Architecture Components
### 1. Inputs (x₁, x₂, ..., xₙ)
- Features or measurable attributes that provide signals for decision making.
Example: For an OR gate implementation:
Inputs: (x₁, x₂) ∈ {0, 1}²

### 2. Weights (w₁, w₂, ..., wₙ)
- Parameters that determine each input's influence on the output. Learned during training through error correction.
- Properties:
- Represent feature importance
- Higher absolute values indicate stronger influence
- Initialized randomly, then optimized

### 3. Bias (b)
- Constant term that shifts the decision boundary independently of inputs.
- Purpose:
- Allows classification when all inputs are zero
- Prevents forcing the boundary through origin
- Controls activation threshold
  
### 4. Weighted Sum (Net Input)
Linear combination of inputs and weights with bias:
z = Σ(wᵢ * xᵢ) + b   for i = 1 to n

### 5. Activation Function
Typically uses a step function to produce binary output:
ŷ = { 1 if z ≥ 0
      0 otherwise }
# Mathematical Model
## Forward Propagation
Given: Input vector x, weight vector w, bias b
1. Compute weighted sum: z = w·x + b
2. Apply activation: ŷ = step(z)
Decision Boundary
The perceptron defines a hyperplane:
w·x + b = 0
All points where w·x + b ≥ 0 are classified as 1, others as 0.

## Training Algorithm
Perceptron Learning Rule
Iteratively updates weights based on misclassification errors.

## Algorithm Steps:
- Initialize weights and bias (typically to zeros or small random values)
- For each training sample (x, y):
- Compute prediction: ŷ = step(w·x + b)
- Calculate error: error = y - ŷ
- Update weights: wᵢ ← wᵢ + η * error * xᵢ
- Update bias: b ← b + η * error
Repeat for multiple epochs until convergence
Where:
η = Learning rate (0 < η ≤ 1),
y = True label (0 or 1),
ŷ = Predicted label

## Convergence Theorem
For linearly separable data, the perceptron algorithm is guaranteed to converge to a solution in finite steps.

## Perceptron vs. Neural Networks
1. Single Perceptron Limitations
2. Only linear decision boundaries
3. Cannot solve XOR problem
4. Limited to binary classification

## Multi-Layer Extension
1. Neural networks overcome limitations by:
2. Multiple Layers: Input, hidden, and output layers
3. Non-linear Activations: Sigmoid, ReLU, Tanh
4. Backpropagation: Efficient gradient-based learning

## Neural Network Layer Computation:
Hidden layer: z⁽¹⁾ = W⁽¹⁾x + b⁽¹⁾, a⁽¹⁾ = σ(z⁽¹⁾)
Output layer: z⁽²⁾ = W⁽²⁾a⁽¹⁾ + b⁽²⁾, ŷ = σ(z⁽²⁾)
